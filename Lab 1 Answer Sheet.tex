\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.2 in}
\setlength{\evensidemargin}{0.2in}
\setlength{\topmargin}{-0.8 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[tbtags]{amsmath}
\newtheorem{thm}{Theorem}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{setspace}

\begin{document}
\begin{center}
\begin{spacing}{1.75}
{\Large { ECE368: Probabilistic Reasoning} \\
	{Lab 1: Classification with Multinomial and Gaussian Models}}
\end{spacing}
\vspace{-1em}
\end{center}
%\vspace{-11mm}
You can complete this lab in a group of two. Please provide the name and student number of both members.
%\begin{center}
%	\framebox{
%	\vbox{
%	\vspace{3mm}
%	\hbox to 6.28in { {\bf Name:
%	\centerline{Student Number:}} }\\[5mm]
%	\hbox to 6.28in { {\bf Name:
%	\centerline{Student Number:}} }
%	\vspace{3mm}}}
%\end{center}
\begin{center}
	\framebox{%
		\vbox{%
			\vskip 3mm
			\hbox to 6.28in{\rlap{\bf Name: Bharat Bhargava}\hfil{\bf Student Number: 1010380892}\hfil}
			\vskip 5mm
			\hbox to 6.28in{\rlap{\bf Name: Daivik Dhar}\hfil{\bf Student Number: 1010214260}\hfil}
			\vskip 3mm
		}%
	}
\end{center}


{\bf You should hand in:} 1) A scanned \textsf{.pdf} version of this sheet with your answers (file size should be under $2$ MB); 2) one figure for Question {\bf 1}.2.(c) and two figures for Question {\bf 2}.1.(c) in the \textsf{.pdf} format; and 3) two Python files \textsf{classifier.py} and \textsf{ldaqda.py} that contain your code. All these files should be uploaded to Quercus.

\section{Na\"{i}ve Bayes Classifier for Spam Filtering}
\begin{enumerate}
\item
     \begin{enumerate}
     \item  Write down the estimators for $p_d$ and
	$q_d$ as functions of the training data $\{\mathbf{x}_n,y_n\}, n=1,2,\ldots,N$ using the technique of ``Laplace smoothing".  ({\bf $1$ pt})

      \begin{center}
   \framebox{
      \begin{minipage}{5.5in}
      \vspace{0.2cm} % Small top padding
      \textbf{Idea:} Laplace smoothing avoids zero probabilities for unseen words by adding a count of 1 to every word observation. 
      Let $S = \{n : y_n = 1\}$ be the spam emails and $H = \{n : y_n = 0\}$ be the ham emails. The estimators are:
      
      \[
      \hat{p}_d = \frac{1 + \sum_{n \in S} x_{n,d}}{D + \sum_{n \in S} \sum_{j=1}^D x_{n,j}}, \quad
      \hat{q}_d = \frac{1 + \sum_{n \in H} x_{n,d}}{D + \sum_{n \in H} \sum_{j=1}^D x_{n,j}}
      \]
      \noindent where $D$ is the vocabulary size and $x_{n,d}$ is the count of word $d$ in email $n$.
      \vspace{0.2cm} % Small bottom padding
    \end{minipage}
   }
   \end{center}
     \item Complete function \textsf{learn\_distributions} in python file \textsf{classifier.py} based on the expressions you derived in part (a). ({\bf $1$ pt})%and upload your code to the system.
     \end{enumerate}
\item
    \begin{enumerate}
    \item
    Write down the MAP rule to decide whether $y=1$ or $y=0$ based on its feature vector $\mathbf{x}$ for a new email $\{\mathbf{x},y\}$. The $d$-th entry of $\mathbf{x}$ is denoted by $x_d$. Please incorporate $p_d$ and $q_d$ in your expression. Please assume that $\pi=0.5$. ({\bf $1$ pt})
     \begin{center}
  \framebox{
    \begin{minipage}{5.5in}
      \vspace{0.2cm}
      The MAP decision rule compares the posterior probabilities. We decide $y=1$ if $P(y=1|\mathbf{x}) > P(y=0|\mathbf{x})$. Using Bayes' rule and the assumption that priors are equal ($\pi = 1-\pi = 0.5$), this simplifies to comparing the likelihoods:
      
      \[
      P(\mathbf{x}|y=1) \underset{y=0}{\overset{y=1}{\gtrless}} P(\mathbf{x}|y=0)
      \]
      
      Substituting the Multinomial model distributions and taking the natural logarithm (to prevent numerical underflow), we get the final decision rule:
      
      \[
      \sum_{d=1}^D x_d \ln(p_d) \underset{y=0}{\overset{y=1}{\gtrless}} \sum_{d=1}^D x_d \ln(q_d)
      \]
      \vspace{0.1cm}
      \hrule width 3cm % Small line to separate the note
      \vspace{0.1cm}
      
      \footnotesize \textbf{Note:} The prior terms $\ln(\pi)$ and $\ln(1-\pi)$ were removed because $\pi = 0.5$, so $\ln(\pi) = \ln(1-\pi)$ cancel out.
      
      \vspace{0.2cm}
    \end{minipage}
  }
\end{center}

    \item Complete function \textsf{classify\_new\_email} in \textsf{classifier.py}, and test the classifier on the testing set. The number of Type $1$ errors is \framebox{\vbox{\vspace{3mm}\hbox to 10mm {\hfill 2 \hfill}}}, and the number of Type $2$ errors is \framebox{\vbox{\vspace{3mm}\hbox to 10mm {\hfill 5 \hfill}}}.

    \item Write down the modified decision rule in the classifier such that
	these two types of error can be traded off. Please introduce a new parameter to achieve such a trade-off.   ({\bf $0.5$ pt})
    \begin{center}
  \framebox{
    \begin{minipage}{5.5in}
      \vspace{0.2cm}
      We introduce a threshold parameter $\tau$ to bias the decision towards one class. The modified decision rule is: Decide $y=1$ (Spam) if:
      
      \[
      \ln P(\mathbf{x}|y=1) + \ln P(y=1) > \ln P(\mathbf{x}|y=0) + \ln P(y=0) + \tau
      \]
      
      \noindent Otherwise, decide $y=0$ (Ham).
      
      \begin{itemize}
          \item Increasing $\tau$ makes it harder to classify as Spam, decreasing Type 2 errors (False Positives) but increasing Type 1 errors.
          \item Decreasing $\tau$ makes it easier to classify as Spam, decreasing Type 1 errors (False Negatives) but increasing Type 2 errors.
      \end{itemize}
      \vspace{0.1cm}
    \end{minipage}
  }
\end{center}

    Write your code in file \textsf{classifier.py} to implement your modified decision rule. Test it on the testing set and plot a figure to show the trade-off between Type $1$ error and Type $2$ error. In the figure, the {\it x}-axis should be the number of Type
		$1$ errors and the {\it y}-axis should be the number of Type $2$
		errors. Plot at least $10$ points corresponding to different pairs of these two types of error in your figure. The two end points of the plot should be: 1) the point with zero Type $1$ error; and 2) the point with zero Type $2$ error. Please save the figure with name \textbf{nbc.pdf}. ({\bf $1$ pt})

        \end{enumerate}

   \item Why do we need Laplace smoothing? Briefly explain what would go wrong if we do use the maximum likelihood estimators in the training process. ({\bf $0.5$ pt})
    \begin{center}
   \framebox{
      \vbox{\vspace{20mm}
    \hbox to 5.5in {}
      \vspace{5.5mm}}
   }
   \end{center}

\end{enumerate}


\end{document}
